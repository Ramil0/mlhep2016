{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## About\n",
    "\n",
    "In this notebook we prepare a simple solution for the [kaggle challenge on higgs.](https://inclass.kaggle.com/c/mlhep-2016-higgs-detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd datasets; wget -O public_train_10000.root -nc --no-check-certificate https://2016.mlhep.yandex.net/data/higgs/public_train_10000.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can download training sample with 100000 available events\n",
    "# uncomment the below row\n",
    "# !cd datasets; wget -O public_train_100000.root -nc --no-check-certificate https://2016.mlhep.yandex.net/data/higgs/public_train_100000.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd datasets; wget -O public_test.root -nc --no-check-certificate https://2016.mlhep.yandex.net/data/higgs/public_test.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! pip install -e git+git://github.com/fmfn/BayesianOptimization#egg=bayes_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jet3_pt', 'jet3_eta', 'm_jjj', 'mem_phi', 'jet1_pt', 'jet4_phi', 'jet1_phi', 'jet2_eta', 'jet3_btag', 'm_jlv', 'm_wbb', 'jet4_pt', 'jet4_btag', 'jet2_pt', 'jet1_btag', 'm_jj', 'm_wwbb', 'jet2_phi', 'lepton_phi', 'm_bb', 'm_lv', 'jet4_eta', 'jet2_btag', 'lepton_pt', 'mem_pt', 'lepton_eta', 'jet3_phi', 'jet1_eta']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import numpy\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import root_numpy\n",
    "data = pandas.DataFrame(root_numpy.root2array('datasets/public_train_10000.root'))\n",
    "test = pandas.DataFrame(root_numpy.root2array('datasets/public_test.root'))\n",
    "\n",
    "features = list(set(data.columns) - {'event_id', 'target'})\n",
    "print (features)\n",
    "\n",
    "high_level_features = ['m_jj', 'm_jjj', 'm_jlv', 'm_wwbb', 'm_bb', 'm_wbb', 'm_lv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "hist_params = {'normed': True, 'bins': 60, 'alpha': 0.4}\n",
    "# create the figure\n",
    "plt.figure(figsize=(16, 25))\n",
    "for n, feature in enumerate(features[0:18]):\n",
    "    # add sub plot on our figure\n",
    "    plt.subplot(len(features) // 5 + 1, 3, n+1)\n",
    "    # define range for histograms by cutting 1% of data from both ends\n",
    "    min_value, max_value = numpy.percentile(data[feature], [1, 99])\n",
    "    plt.hist(data.ix[data.target.values == 0, feature].values, range=(min_value, max_value), \n",
    "             label='class 0', **hist_params)\n",
    "    plt.hist(data.ix[data.target.values == 1, feature].values, range=(min_value, max_value), \n",
    "             label='class 1', **hist_params)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 115)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def limitPerc(X, l,h):\n",
    "    Z = X.copy()\n",
    "    for feature in Z:\n",
    "        upperlim = np.percentile(Z[feature],h)\n",
    "        Z[feature][ Z[feature] > upperlim ] = upperlim\n",
    "        lowerlim = np.percentile(Z[feature],l)\n",
    "        Z[feature][ Z[feature] < lowerlim ] = lowerlim\n",
    "    return Z\n",
    "        \n",
    "def centralize(X):\n",
    "    Z = X.copy()\n",
    "    for feature in Z:\n",
    "        Z[feature] -= np.mean(Z[feature])\n",
    "    return Z\n",
    "\n",
    "def rescale(X):\n",
    "    Z = X.copy()\n",
    "    for f in Z:\n",
    "        Z[f] /= max(X[f]) - min(X[f])\n",
    "    return Z\n",
    "\n",
    "def logarize(X):\n",
    "    Z = X.copy()\n",
    "    Z = np.log(X - min(X) + 1)\n",
    "    return Z\n",
    "\n",
    "def angleBetween(phi1, eta1,phi2,eta2):\n",
    "    return np.arccos(np.cos(eta1)*np.cos(eta2) + np.sin(eta1)*np.sin(eta2)*np.cos(phi1-phi2))\n",
    "\n",
    "def preprocess(X):\n",
    "    Z = X.copy()\n",
    "    Z = limitPerc(Z,0.5,99.5)\n",
    "    Z['log_m_lv'] = np.log(Z['m_lv'] - min(Z['m_lv']) + 1)\n",
    "    Z['log_m_jj'] = np.log(Z['m_jj'] - min(Z['m_jj']) + 1)\n",
    "    Z['log_m_jjj'] = np.log(Z['m_jjj'] - min(Z['m_jjj']) + 1)\n",
    "    Z['log_m_wwbb'] = logarize(Z['m_wwbb'])\n",
    "    Z['sqrt_jet1_pt'] = np.sqrt(Z['jet1_pt'] - min(Z['jet1_pt'])+0)\n",
    "    Z['sqrt_jet2_pt'] = np.sqrt(Z['jet2_pt'] - min(Z['jet2_pt'])+0)\n",
    "    Z['sqrt_jet3_pt'] = np.sqrt(Z['jet3_pt'] - min(Z['jet3_pt'])+0)\n",
    "    Z['sqrt_jet4_pt'] = np.sqrt(Z['jet4_pt'] - min(Z['jet4_pt'])+0)\n",
    "    \n",
    "    for m1 in high_level_features:\n",
    "        for m2 in high_level_features:\n",
    "            if(m1 != m2):\n",
    "                Z[m1+'div'+m2] = Z[m1]/Z[m2]\n",
    "                #asfasdfadsfsadf=1\n",
    "\n",
    "    for m1 in high_level_features:\n",
    "        for m2 in high_level_features:\n",
    "            if(m1 > m2):\n",
    "                Z[m1+'times'+m2] = Z[m1]*Z[m2]\n",
    "                #asdfasfadsf=1\n",
    "                \n",
    "    phis = ['jet1_phi','jet2_phi','jet3_phi','jet4_phi']\n",
    "    etas = ['jet1_eta','jet2_eta','jet3_eta','jet4_eta']\n",
    "    angles = []\n",
    "    \n",
    "    for j1 in range(len(phis)):\n",
    "        for j2 in range(j1+1,len(phis)):\n",
    "            angl = angleBetween(Z[phis[j1]],Z[etas[j1]],Z[phis[j2]],Z[etas[j2]])\n",
    "            Z['angle'+str(j1)+str(j2)] = angl\n",
    "            angles.append(angl)\n",
    "            \n",
    "    Z['max/minangle'] = np.amax(angles, axis=0) / np.amin(angles, axis=0) \n",
    "    Z['maxangle'] = np.amax(angles, axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    jet_pairs = frozenset(filter(lambda pair:len(pair) == 2, map(frozenset, product(range(1,5), range(1,5)))))\n",
    "\n",
    "    distances = []\n",
    "    for jet_pair in jet_pairs:\n",
    "        jet1, jet2 = jet_pair\n",
    "        distances.append(np.sqrt(\n",
    "           (Z[\"jet%d_eta\" % jet1] - Z[\"jet%d_eta\" % jet2])**2 +\n",
    "           (Z[\"jet%d_phi\" % jet1] - Z[\"jet%d_phi\" % jet2])**2))\n",
    "    distances_df = pandas.concat(distances, axis=1)\n",
    "    distances_df.columns=list(map(lambda pair: \"d^2(%d, %d)\" % tuple(pair), jet_pairs))\n",
    "    Z = pandas.concat(\n",
    "        [Z, distances_df], axis=1)\n",
    "            \n",
    "    \n",
    "           \n",
    "    \n",
    "    #Z['sin_jet1_phi'] = np.cos(Z['jet1_phi'] )\n",
    "    #Z['sin_jet2_phi'] = np.cos(Z['jet2_phi'] )\n",
    "    #Z['sin_jet3_phi'] = np.cos(Z['jet3_phi'] )\n",
    "    #Z['sin_jet4_phi'] = np.cos(Z['jet4_phi'] )\n",
    "    #Z['sin_mem_phi'] = np.cos(Z['mem_phi'] )\n",
    "    #Z['sin_lepton_phi'] = np.sin(Z['lepton_phi'] )\n",
    "    #Z['jet12_phi'] = Z['jet1_phi'] / Z['jet2_phi']\n",
    "    #Z['jet13_phi'] = Z['jet1_phi'] / Z['jet3_phi']\n",
    "    #Z['jet14_phi'] = Z['jet1_phi'] / Z['jet4_phi']\n",
    "    #Z['jet23_phi'] = Z['jet2_phi'] / Z['jet3_phi']\n",
    "    #Z['jet24_phi'] = Z['jet2_phi'] / Z['jet4_phi']\n",
    "    #Z['jet34_phi'] = Z['jet3_phi'] / Z['jet4_phi']\n",
    "    #del(Z['m_lv'])\n",
    "    #del(Z['m_wwbb'])\n",
    "    #del(Z['jet1_pt'])\n",
    "    #del(Z['jet2_pt'])\n",
    "    #del(Z['jet3_pt'])\n",
    "    #del(Z['jet4_pt'])\n",
    "    return centralize(Z)\n",
    "\n",
    "preprocess(data[0:1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature='sqrt_jet1_pt'\n",
    "# add sub plot on our figure\n",
    "# define range for histograms by cutting 1% of data from both ends\n",
    "min_value, max_value = numpy.percentile(Z[feature], [1, 99])\n",
    "plt.hist(Z.ix[data.target.values == 0, feature].values, range=(min_value, max_value), \n",
    "         label='class 0', **hist_params)\n",
    "plt.hist(Z.ix[data.target.values == 1, feature].values, range=(min_value, max_value), \n",
    "         label='class 1', **hist_params)\n",
    "plt.legend(loc='best')\n",
    "plt.title(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms for each high-level feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide training data into 2 parts \n",
    "`train_test_split` function is used to divide into 2 parts to preserve quality overestimating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = train_test_split(data, random_state=17, train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1\n",
    "results_2 = np.zeros(N)\n",
    "for i in xrange(N):\n",
    "    from sklearn.decomposition import PCA\n",
    "    X = training_data[features]\n",
    "    pca = PCA(n_components=70)\n",
    "    pca.fit(preprocess(X))\n",
    "    #print(pca.explained_variance_ratio_) \n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from rep.estimators import XGBoostClassifier\n",
    "    classif =  RandomForestClassifier(n_estimators=100, max_depth=10, max_features=50, n_jobs=-1)\n",
    "    classif.fit(pca.transform(preprocess(training_data[features])).astype(np.float64), training_data.target)\n",
    "\n",
    "    # predict validation sample (probability for each event)\n",
    "    proba = classif.predict_proba(pca.transform(preprocess(validation_data[features])).astype(np.float64))\n",
    "\n",
    "    # take probability to be 1 class to compute ROC AUC\n",
    "    results_2[i] = roc_auc_score(validation_data.target, proba[:, 1])\n",
    "    print (results_2[i])\n",
    "print (sorted(results_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767325075326\n",
      "0.763440373752\n",
      "0.765936676549\n",
      "0.765662213938\n",
      "0.766528832147\n",
      "0.766082453394\n",
      "0.768209789971\n",
      "0.765919585471\n",
      "0.767202421705\n",
      "0.765524479954\n",
      "[0.76344037375172291, 0.76552447995367312, 0.76566221393807599, 0.76591958547097494, 0.7659366765493314, 0.76608245339413739, 0.7665288321465088, 0.76720242170526731, 0.76732507532641447, 0.76820978997075418]\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "results_2 = np.zeros(N)\n",
    "for i in xrange(N):\n",
    "    from sklearn.decomposition import PCA\n",
    "    X = training_data[features]\n",
    "    pca = PCA(n_components=70)\n",
    "    pca.fit(preprocess(X))\n",
    "    #print(pca.explained_variance_ratio_) \n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from rep.estimators import XGBoostClassifier\n",
    "    #classif = XGBoostClassifier()\n",
    "    classif =  RandomForestClassifier(n_estimators=100, max_depth=10, max_features=50, n_jobs=-1)\n",
    "    classif.fit(pca.transform(preprocess(training_data[features])).astype(np.float64), training_data.target)\n",
    "\n",
    "    # predict validation sample (probability for each event)\n",
    "    proba = classif.predict_proba(pca.transform(preprocess(validation_data[features])).astype(np.float64))\n",
    "\n",
    "    # take probability to be 1 class to compute ROC AUC\n",
    "    results_2[i] = roc_auc_score(validation_data.target, proba[:, 1])\n",
    "    print (results_2[i])\n",
    "print (sorted(results_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=6)\n",
    "pca.fit(preprocess(X))\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |       eta |   max_depth |   subsample | \n",
      "    1 | 00m00s | \u001b[35m   0.59505\u001b[0m | \u001b[32m   0.6199\u001b[0m | \u001b[32m     4.6578\u001b[0m | \u001b[32m     0.0958\u001b[0m | \n",
      "    2 | 00m02s | \u001b[35m   0.71623\u001b[0m | \u001b[32m   0.2512\u001b[0m | \u001b[32m    27.6212\u001b[0m | \u001b[32m     0.8456\u001b[0m | \n",
      "    3 | 00m00s |    0.69185 |    0.4551 |      1.3938 |      0.5658 | \n",
      "    4 | 00m00s |    0.68181 |    0.5197 |      2.4860 |      0.7057 | \n",
      "    5 | 00m03s | \u001b[35m   0.72792\u001b[0m | \u001b[32m   0.0131\u001b[0m | \u001b[32m    22.2196\u001b[0m | \u001b[32m     0.6223\u001b[0m | \n",
      "    6 | 00m01s |    0.71578 |    0.4689 |      6.3808 |      0.6823 | \n",
      "    7 | 00m01s |    0.67131 |    0.6713 |     27.1614 |      0.4644 | \n",
      "    8 | 00m00s |    0.67562 |    0.6899 |      1.3238 |      0.9474 | \n",
      "    9 | 00m01s |    0.62332 |    0.8962 |     25.4290 |      0.3881 | \n",
      "   10 | 00m02s |    0.72521 |    0.0811 |     23.6617 |      0.5608 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |       eta |   max_depth |   subsample | \n",
      "   11 | 00m19s |    0.70944 |    0.2961 |     15.0420 |      0.6752 | \n",
      "   12 | 00m12s |    0.71875 |    0.1006 |     26.1299 |      0.6945 | \n",
      "   13"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "datalimit = 1000\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "def rfccv( subsample, max_depth,eta):\n",
    "    return cross_val_score(XGBoostClassifier(eta=eta,subsample=subsample,\n",
    "            n_estimators=100,\n",
    "            max_depth=int(max_depth),\n",
    "            random_state=2),\n",
    "                           pca.transform(preprocess(data[features][0:datalimit])).astype(np.float64), data.target[0:datalimit], scoring=\"roc_auc\",\n",
    "                           cv=4, n_jobs=1).mean()\n",
    "\n",
    "rfcBO = BayesianOptimization(rfccv, {'subsample': (0, 1),\n",
    "                                     'max_depth': (1, 30),\n",
    "                                     'eta': (0, 1)\n",
    "                                    })\n",
    "rfcBO.maximize(init_points=10, n_iter=50)\n",
    "print('-'*53)\n",
    "print('Final Results')\n",
    "print('RFC: %f' % rfcBO.res['max']['max_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "datalimit = 1000\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "def rfccv(max_features, min_samples_split, max_depth, criterion):\n",
    "    return cross_val_score(RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            max_depth=int(max_depth),\n",
    "            max_features=int(max_features),\n",
    "            criterion=('gini' if criterion <= 0 else 'entropy'),\n",
    "            random_state=2),\n",
    "                           pca.transform(preprocess(data[features][0:datalimit])), data.target[0:datalimit], scoring=\"roc_auc\",\n",
    "                           cv=4, n_jobs=4).mean()\n",
    "\n",
    "rfcBO = BayesianOptimization(rfccv, {'max_features': (10, 70),\n",
    "                                     'min_samples_split': (2, 20),\n",
    "                                     'max_depth': (1, 50),\n",
    "                                     'criterion': (-1, 1)})\n",
    "rfcBO.maximize(init_points=10, n_iter=50)\n",
    "print('-'*53)\n",
    "print('Final Results')\n",
    "print('RFC: %f' % rfcBO.res['max']['max_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n",
      "[0.73080003844798314, 0.74286656372643634, 0.754151788586502, 0.75605555733555474, 0.7602497909026571, 0.7641015987952966, 0.76659270769920873, 0.76785828714235371, 0.76895145724763547, 0.77785015762722776]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "N = 10\n",
    "\n",
    "\n",
    "pca = PCA(n_components=113)\n",
    "pca.fit(preprocess(data[features]))\n",
    "#print(pca.explained_variance_ratio_) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from rep.estimators import XGBoostClassifier\n",
    "from rep.estimators.tmva import TMVAClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#classif =  RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1)\n",
    "classif = XGBoostClassifier()\n",
    "\n",
    "print (\"Validating\")\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "results_1 = cross_val_score(classif,pca.transform(preprocess(data[features])).astype(np.float64),data.target,scoring=\"roc_auc\",cv=N, n_jobs=N)\n",
    "print (sorted(results_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+70\n",
      "-30\n",
      "40.0\n",
      "diff: 0.0106447953203\n",
      "std:  0.0130603978996\n",
      "0.758947794751\n"
     ]
    }
   ],
   "source": [
    "better = 0\n",
    "worse = 0\n",
    "for i in xrange(N):\n",
    "    for j in xrange(N):\n",
    "        if(results_1[i] > results_2[j]):\n",
    "            better = better +1\n",
    "        if(results_1[i] < results_2[j]): \n",
    "            worse =worse + 1\n",
    "print '+'+str(better)\n",
    "print '-'+str(worse)\n",
    "print (2.0*better/(better+worse)-1)*100\n",
    "print 'diff: ' + str((np.mean(results_1) - np.mean(results_2)))\n",
    "print 'std:  ' + str(np.std(results_1))\n",
    "print (np.mean(results_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_2 = results_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict test sample\n",
    "kaggle_proba = classif.predict_proba(test[high_level_features])[:, 1]\n",
    "kaggle_ids = test.event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "def create_solution(ids, proba, filename='baseline.csv'):\n",
    "    \"\"\"saves predictions to file and provides a link for downloading \"\"\"\n",
    "    pandas.DataFrame({'event_id': ids, 'prediction': proba}).to_csv('datasets/{}'.format(filename), index=False)\n",
    "    return FileLink('datasets/{}'.format(filename))\n",
    "    \n",
    "create_solution(kaggle_ids, kaggle_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import offsetbox\n",
    "def plot_embedding(X,y,ax=None,show_images=True,min_dist=1e-2):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i]),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if not show_images:\n",
    "        return\n",
    "    if ax is None:\n",
    "        ax = plt.subplot(1,1,1)\n",
    "        \n",
    "    shown_images = np.array([[1., 1.]])  # just something big\n",
    "    for i in range(X.shape[0]):\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < min_dist: continue\n",
    "        shown_images = np.r_[shown_images, [X[i]]]\n",
    "        imagebox = offsetbox.AnnotationBbox(\n",
    "            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "            X[i])\n",
    "        ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = rescale(preprocess(training_data[features][0:1000]))\n",
    "#X = ((training_data[features][0:1000]))\n",
    "y = training_data.target[0:1000]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2,verbose=2,perplexity=50)\n",
    "Xtsne = tsne.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(Xtsne[:,0],Xtsne[:,1],c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2,verbose=2,n_init=1)\n",
    "Xmds = mds.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(Xmds[:,0],Xmds[:,1],c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=3).fit(X)\n",
    "Xpca = pca2.transform(X)\n",
    "plt.scatter(Xpca[:,2],Xpca[:,1],c=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
